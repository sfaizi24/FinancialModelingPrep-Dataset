{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb689cb",
   "metadata": {},
   "source": [
    "## 1. Consolidate Symbols from Price Data\n",
    "\n",
    "Before fetching market cap data, we need a definitive list of symbols for which we already have price data. This script will scan the `partials_price` directory, extract all the unique symbols from the filenames, and save them into a new list. This ensures we only query the market cap API for symbols that are relevant to our existing dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8ed5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10798 unique symbols with price data.\n",
      "Symbol list for market cap fetching saved to 'symbols_for_market_cap.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_symbol_list_from_price_partials():\n",
    "    \"\"\"\n",
    "    Scans the price partials directory to create a unique list of symbols\n",
    "    that we have price data for.\n",
    "    \"\"\"\n",
    "    partials_dir = '../Prices/Price Data/partials_price'\n",
    "    output_path = 'symbols_for_market_cap.csv'\n",
    "    \n",
    "    if not os.path.isdir(partials_dir):\n",
    "        print(f\"Error: Price partials directory not found at '{partials_dir}'.\")\n",
    "        return\n",
    "\n",
    "    # Get symbol from each filename in the directory\n",
    "    all_symbols = [f.split('.')[0] for f in os.listdir(partials_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    if not all_symbols:\n",
    "        print(\"No price data files found in the partials directory.\")\n",
    "        return\n",
    "        \n",
    "    # Create a DataFrame with unique symbols and save to CSV\n",
    "    symbols_df = pd.DataFrame(list(set(all_symbols)), columns=['symbol'])\n",
    "    symbols_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Found {len(symbols_df)} unique symbols with price data.\")\n",
    "    print(f\"Symbol list for market cap fetching saved to '{output_path}'.\")\n",
    "\n",
    "# Run the function\n",
    "create_symbol_list_from_price_partials()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867fd191",
   "metadata": {},
   "source": [
    "## 2. Fetch Historical Market Cap Data\n",
    "\n",
    "This script fetches the historical daily market cap for every symbol in our generated list. It's built to be robust and efficient, using the same asynchronous, resumable approach as the price and financial statement fetchers.\n",
    "\n",
    "- **Concurrent:** It uses `aiohttp` to make up to 15 API requests at a time.\n",
    "- **Resumable:** It saves data for each symbol to a file in the `partials` directory. If the script is stopped, it will automatically skip any symbols it has already completed.\n",
    "- **Error Handling:** It will retry failed requests and, if a symbol consistently fails, it will be logged for review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API fetch for 10798 symbols.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc328f24c5b94df8900ac0a82132ef24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching Market Caps:   0%|          | 0/10798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for ENB. Waiting 15s...\n",
      "Rate limit hit for TELO. Waiting 15s...\n",
      "Rate limit hit for NODK. Waiting 15s...\n",
      "Rate limit hit for WFBI. Waiting 15s...\n",
      "Rate limit hit for CONN. Waiting 15s...\n",
      "Rate limit hit for CVE. Waiting 15s...\n",
      "Rate limit hit for GCBC. Waiting 15s...\n",
      "Rate limit hit for LLY. Waiting 15s...\n",
      "Rate limit hit for ERII. Waiting 15s...\n",
      "Rate limit hit for NYMX. Waiting 15s...\n",
      "Rate limit hit for MGEN. Waiting 15s...\n",
      "Rate limit hit for HEP. Waiting 15s...\n",
      "Rate limit hit for BANC-PE. Waiting 15s...\n",
      "Rate limit hit for AIRG. Waiting 15s...\n",
      "Rate limit hit for APXTU. Waiting 15s...\n",
      "Rate limit hit for ENB. Waiting 15s...\n",
      "Rate limit hit for TELO. Waiting 15s...\n",
      "Rate limit hit for WFBI. Waiting 15s...\n",
      "Rate limit hit for NODK. Waiting 15s...\n",
      "Rate limit hit for GCBC. Waiting 15s...\n",
      "Rate limit hit for CONN. Waiting 15s...\n",
      "Rate limit hit for CVE. Waiting 15s...\n",
      "Rate limit hit for LLY. Waiting 15s...\n",
      "Rate limit hit for ERII. Waiting 15s...\n",
      "Rate limit hit for HEP. Waiting 15s...\n",
      "Rate limit hit for NYMX. Waiting 15s...\n",
      "Rate limit hit for BANC-PE. Waiting 15s...\n",
      "Rate limit hit for MGEN. Waiting 15s...\n",
      "Rate limit hit for AIRG. Waiting 15s...\n",
      "Rate limit hit for APXTU. Waiting 15s...\n",
      "\\n--- API data fetching is complete. ---\n",
      "Identified 266 symbols that could not be fetched. Their list has been saved to 'symbols_with_no_market_cap.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_market_cap_and_save(session, symbol, pbar, partials_dir, api_key):\n",
    "    \"\"\"Fetches, processes, and saves daily market cap for a single symbol from the API.\"\"\"\n",
    "    ten_years_ago = (datetime.now() - timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://financialmodelingprep.com/stable/historical-market-capitalization\"\n",
    "    params = {\"symbol\": symbol, \"from\": ten_years_ago, \"to\": today, \"apikey\": api_key}\n",
    "    \n",
    "    max_retries = 3\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=60) as response:\n",
    "                if response.status == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", 15))\n",
    "                    pbar.write(f\"Rate limit hit for {symbol}. Waiting {retry_after}s...\")\n",
    "                    await asyncio.sleep(retry_after)\n",
    "                    continue\n",
    "\n",
    "                response.raise_for_status()\n",
    "                data = await response.json()\n",
    "                \n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    if not df.empty:\n",
    "                        output_path = os.path.join(partials_dir, f\"{symbol}.csv\")\n",
    "                        df.to_csv(output_path, index=False)\n",
    "                    return # Success\n",
    "                else:\n",
    "                    # No data found, but request was successful\n",
    "                    return\n",
    "\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            if attempts < max_retries:\n",
    "                wait_time = 2 * attempts\n",
    "                pbar.write(f\"Request for {symbol} failed with {e}. Retrying in {wait_time}s... (Attempt {attempts}/{max_retries})\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                pbar.write(f\"API fetch failed for {symbol} after {max_retries} retries. Final error: {e}\")\n",
    "    return\n",
    "\n",
    "async def fetch_market_cap_main():\n",
    "    \"\"\"Main function to fetch API data for symbols with rate limiting.\"\"\"\n",
    "    # --- Configuration ---\n",
    "    SYMBOL_LIST_PATH = 'symbols_for_market_cap.csv'\n",
    "    PARTIALS_DIR = 'Market Cap Data/partials'\n",
    "    FAILED_SYMBOLS_PATH = 'symbols_with_no_market_cap.csv'\n",
    "    \n",
    "    os.makedirs(PARTIALS_DIR, exist_ok=True)\n",
    "    \n",
    "    load_dotenv()\n",
    "    API_KEY = os.getenv('API_KEY')\n",
    "    if not API_KEY:\n",
    "        print(\"API_KEY not found in .env file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        symbols_to_fetch_df = pd.read_csv(SYMBOL_LIST_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"'{SYMBOL_LIST_PATH}' not found. Please run the symbol consolidation cell first.\")\n",
    "        return\n",
    "        \n",
    "    all_symbols = symbols_to_fetch_df['symbol'].unique().tolist()\n",
    "    completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    symbols_to_process = [s for s in all_symbols if s not in completed_symbols]\n",
    "\n",
    "    if not symbols_to_process:\n",
    "        print(\"All required symbols have already been processed. No API calls needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting API fetch for {len(symbols_to_process)} symbols.\")\n",
    "    \n",
    "    # --- Rate Limiting and Concurrency Control ---\n",
    "    # To stay under the 2500 requests/minute limit, we'll target 40 requests/second.\n",
    "    # This means we should start a new request no more frequently than every 1/40 = 0.025 seconds.\n",
    "    API_CALL_INTERVAL = 1.0 / 40.0\n",
    "    \n",
    "    # The semaphore controls how many requests can be active at once.\n",
    "    # This prevents overwhelming the server with connections and manages local resource usage.\n",
    "    CONCURRENCY_LIMIT = 15\n",
    "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        with tqdm(total=len(symbols_to_process), desc=\"Fetching Market Caps\") as pbar:\n",
    "            \n",
    "            async def fetch_with_sem(symbol):\n",
    "                # This wrapper function acquires the semaphore before making the API call.\n",
    "                async with sem:\n",
    "                    await fetch_market_cap_and_save(session, symbol, pbar, PARTIALS_DIR, API_KEY)\n",
    "                pbar.update(1)\n",
    "\n",
    "            tasks = []\n",
    "            for symbol in symbols_to_process:\n",
    "                # Create a task for each symbol...\n",
    "                task = asyncio.create_task(fetch_with_sem(symbol))\n",
    "                tasks.append(task)\n",
    "                # ...and then pause briefly before creating the next one.\n",
    "                await asyncio.sleep(API_CALL_INTERVAL)\n",
    "            \n",
    "            # Wait for all the created tasks to complete.\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"\\\\n--- API data fetching is complete. ---\")\n",
    "\n",
    "    # Check for any symbols that failed and output them\n",
    "    final_completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    failed_symbols = [s for s in all_symbols if s not in final_completed_symbols]\n",
    "\n",
    "    if failed_symbols:\n",
    "        failed_df = pd.DataFrame(failed_symbols, columns=['symbol'])\n",
    "        failed_df.to_csv(FAILED_SYMBOLS_PATH, index=False)\n",
    "        print(f\"Identified {len(failed_symbols)} symbols that could not be fetched. Their list has been saved to '{FAILED_SYMBOLS_PATH}'.\")\n",
    "    else:\n",
    "        print(\"All symbols were fetched successfully.\")\n",
    "\n",
    "# Run the API fetching process\n",
    "await fetch_market_cap_main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d138c6",
   "metadata": {},
   "source": [
    "## 3. Assemble Final Market Cap Reports\n",
    "\n",
    "Once the fetching script is complete, run this final cell. It will gather all the individual symbol files from the `partials` directory and assemble them into four final, alphabetized CSV files inside the `Market Cap Data` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a63b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data from 10532 individual symbol files...\n",
      "Saved chunk 'A-F' with 5206722 records to Market Cap Data\\market_cap_data_A-F.csv\n",
      "Saved chunk 'G-L' with 2494634 records to Market Cap Data\\market_cap_data_G-L.csv\n",
      "Saved chunk 'M-R' with 3223399 records to Market Cap Data\\market_cap_data_M-R.csv\n",
      "Saved chunk 'S-Z' with 3036302 records to Market Cap Data\\market_cap_data_S-Z.csv\n",
      "--- Assembly of final market cap reports is complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def assemble_market_cap_reports():\n",
    "    \"\"\"\n",
    "    Assembles individual symbol market cap CSVs from the 'partials' directory\n",
    "    into four final alphabetized batch files.\n",
    "    \"\"\"\n",
    "    partials_dir = 'Market Cap Data/partials'\n",
    "    output_dir = 'Market Cap Data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(partials_dir):\n",
    "        print(f\"Directory with partial files not found: {partials_dir}\")\n",
    "        return\n",
    "\n",
    "    all_files = glob.glob(os.path.join(partials_dir, '*.csv'))\n",
    "    if not all_files:\n",
    "        print(\"No partial symbol files were found to assemble.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Assembling data from {len(all_files)} individual symbol files...\")\n",
    "    \n",
    "    # Use a generator to save memory\n",
    "    df_generator = (pd.read_csv(f) for f in all_files)\n",
    "    full_df = pd.concat(df_generator, ignore_index=True)\n",
    "    \n",
    "    if 'symbol' not in full_df.columns:\n",
    "        print(\"Error: 'symbol' column not found in the assembled data. Cannot create batches.\")\n",
    "        return\n",
    "\n",
    "    # Ensure symbol column is string for sorting\n",
    "    full_df['symbol'] = full_df['symbol'].astype(str)\n",
    "\n",
    "    # Split into alphabetical chunks\n",
    "    symbol_chunks = {\n",
    "        \"A-F\": full_df[full_df['symbol'].str[0].str.upper() <= 'F'],\n",
    "        \"G-L\": full_df[(full_df['symbol'].str[0].str.upper() >= 'G') & (full_df['symbol'].str[0].str.upper() <= 'L')],\n",
    "        \"M-R\": full_df[(full_df['symbol'].str[0].str.upper() >= 'M') & (full_df['symbol'].str[0].str.upper() <= 'R')],\n",
    "        \"S-Z\": full_df[full_df['symbol'].str[0].str.upper() >= 'S']\n",
    "    }\n",
    "\n",
    "    for chunk_name, chunk_df in symbol_chunks.items():\n",
    "        if not chunk_df.empty:\n",
    "            output_path = os.path.join(output_dir, f\"market_cap_data_{chunk_name}.csv\")\n",
    "            chunk_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved chunk '{chunk_name}' with {len(chunk_df)} records to {output_path}\")\n",
    "        else:\n",
    "            print(f\"No data for chunk '{chunk_name}'.\")\n",
    "    \n",
    "    print(\"--- Assembly of final market cap reports is complete. ---\")\n",
    "\n",
    "# Run the assembly function\n",
    "assemble_market_cap_reports()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
