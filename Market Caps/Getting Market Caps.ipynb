{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb689cb",
   "metadata": {},
   "source": [
    "## 1. Consolidate Symbols from All Price Data\n",
    "\n",
    "Before fetching market cap data, we need a definitive list of symbols for which we have any price data. This script will scan both the `partials_price` (last 10 years) and `partials_price_30yr` (10-30 years ago) directories, extract all unique symbols, and save them into a combined list. This ensures we query the market cap API for every symbol relevant to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ed5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10798 unique symbols with price data.\n",
      "Symbol list for market cap fetching saved to 'symbols_for_market_cap.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_symbol_list_from_price_partials():\n",
    "    \"\"\"\n",
    "    Scans all price partials directories (10-year and 30-year) to create a \n",
    "    unique list of symbols that we have price data for.\n",
    "    \"\"\"\n",
    "    partials_dirs = [\n",
    "        '../Prices/Price Data/partials_price',\n",
    "        '../Prices/Price Data/partials_price_30yr'\n",
    "    ]\n",
    "    output_path = 'symbols_for_market_cap.csv'\n",
    "    \n",
    "    all_symbols = set()\n",
    "\n",
    "    for partials_dir in partials_dirs:\n",
    "        if not os.path.isdir(partials_dir):\n",
    "            print(f\"Warning: Price partials directory not found at '{partials_dir}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Get symbol from each filename in the directory\n",
    "        symbols_in_dir = [f.split('.')[0] for f in os.listdir(partials_dir) if f.endswith('.csv')]\n",
    "        all_symbols.update(symbols_in_dir)\n",
    "\n",
    "    if not all_symbols:\n",
    "        print(\"No price data files found in any partials directory.\")\n",
    "        return\n",
    "        \n",
    "    # Create a DataFrame with unique symbols and save to CSV\n",
    "    symbols_df = pd.DataFrame(list(all_symbols), columns=['symbol'])\n",
    "    symbols_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Found {len(symbols_df)} unique symbols with price data across all periods.\")\n",
    "    print(f\"Symbol list for market cap fetching saved to '{output_path}'.\")\n",
    "\n",
    "# Run the function\n",
    "create_symbol_list_from_price_partials()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867fd191",
   "metadata": {},
   "source": [
    "## 2. Fetch Historical Market Cap Data\n",
    "\n",
    "This script fetches the historical daily market cap for every symbol in our generated list. It's built to be robust and efficient, using the same asynchronous, resumable approach as the price and financial statement fetchers.\n",
    "\n",
    "- **Concurrent:** It uses `aiohttp` to make up to 15 API requests at a time.\n",
    "- **Resumable:** It saves data for each symbol to a file in the `partials` directory. If the script is stopped, it will automatically skip any symbols it has already completed.\n",
    "- **Error Handling:** It will retry failed requests and, if a symbol consistently fails, it will be logged for review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API fetch for 10798 symbols.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc328f24c5b94df8900ac0a82132ef24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching Market Caps:   0%|          | 0/10798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for ENB. Waiting 15s...\n",
      "Rate limit hit for TELO. Waiting 15s...\n",
      "Rate limit hit for NODK. Waiting 15s...\n",
      "Rate limit hit for WFBI. Waiting 15s...\n",
      "Rate limit hit for CONN. Waiting 15s...\n",
      "Rate limit hit for CVE. Waiting 15s...\n",
      "Rate limit hit for GCBC. Waiting 15s...\n",
      "Rate limit hit for LLY. Waiting 15s...\n",
      "Rate limit hit for ERII. Waiting 15s...\n",
      "Rate limit hit for NYMX. Waiting 15s...\n",
      "Rate limit hit for MGEN. Waiting 15s...\n",
      "Rate limit hit for HEP. Waiting 15s...\n",
      "Rate limit hit for BANC-PE. Waiting 15s...\n",
      "Rate limit hit for AIRG. Waiting 15s...\n",
      "Rate limit hit for APXTU. Waiting 15s...\n",
      "Rate limit hit for ENB. Waiting 15s...\n",
      "Rate limit hit for TELO. Waiting 15s...\n",
      "Rate limit hit for WFBI. Waiting 15s...\n",
      "Rate limit hit for NODK. Waiting 15s...\n",
      "Rate limit hit for GCBC. Waiting 15s...\n",
      "Rate limit hit for CONN. Waiting 15s...\n",
      "Rate limit hit for CVE. Waiting 15s...\n",
      "Rate limit hit for LLY. Waiting 15s...\n",
      "Rate limit hit for ERII. Waiting 15s...\n",
      "Rate limit hit for HEP. Waiting 15s...\n",
      "Rate limit hit for NYMX. Waiting 15s...\n",
      "Rate limit hit for BANC-PE. Waiting 15s...\n",
      "Rate limit hit for MGEN. Waiting 15s...\n",
      "Rate limit hit for AIRG. Waiting 15s...\n",
      "Rate limit hit for APXTU. Waiting 15s...\n",
      "\\n--- API data fetching is complete. ---\n",
      "Identified 266 symbols that could not be fetched. Their list has been saved to 'symbols_with_no_market_cap.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_market_cap_and_save(session, symbol, pbar, partials_dir, api_key):\n",
    "    \"\"\"Fetches, processes, and saves daily market cap for a single symbol from the API.\"\"\"\n",
    "    ten_years_ago = (datetime.now() - timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://financialmodelingprep.com/stable/historical-market-capitalization\"\n",
    "    params = {\"symbol\": symbol, \"from\": ten_years_ago, \"to\": today, \"apikey\": api_key}\n",
    "    \n",
    "    max_retries = 3\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=60) as response:\n",
    "                if response.status == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", 15))\n",
    "                    pbar.write(f\"Rate limit hit for {symbol}. Waiting {retry_after}s...\")\n",
    "                    await asyncio.sleep(retry_after)\n",
    "                    continue\n",
    "\n",
    "                response.raise_for_status()\n",
    "                data = await response.json()\n",
    "                \n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    if not df.empty:\n",
    "                        output_path = os.path.join(partials_dir, f\"{symbol}.csv\")\n",
    "                        df.to_csv(output_path, index=False)\n",
    "                    return # Success\n",
    "                else:\n",
    "                    # No data found, but request was successful\n",
    "                    return\n",
    "\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            if attempts < max_retries:\n",
    "                wait_time = 2 * attempts\n",
    "                pbar.write(f\"Request for {symbol} failed with {e}. Retrying in {wait_time}s... (Attempt {attempts}/{max_retries})\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                pbar.write(f\"API fetch failed for {symbol} after {max_retries} retries. Final error: {e}\")\n",
    "    return\n",
    "\n",
    "async def fetch_market_cap_main():\n",
    "    \"\"\"Main function to fetch API data for symbols with rate limiting.\"\"\"\n",
    "    # --- Configuration ---\n",
    "    SYMBOL_LIST_PATH = 'symbols_for_market_cap.csv'\n",
    "    PARTIALS_DIR = 'Market Cap Data/partials'\n",
    "    FAILED_SYMBOLS_PATH = 'symbols_with_no_market_cap.csv'\n",
    "    \n",
    "    os.makedirs(PARTIALS_DIR, exist_ok=True)\n",
    "    \n",
    "    load_dotenv()\n",
    "    API_KEY = os.getenv('API_KEY')\n",
    "    if not API_KEY:\n",
    "        print(\"API_KEY not found in .env file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        symbols_to_fetch_df = pd.read_csv(SYMBOL_LIST_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"'{SYMBOL_LIST_PATH}' not found. Please run the symbol consolidation cell first.\")\n",
    "        return\n",
    "        \n",
    "    all_symbols = symbols_to_fetch_df['symbol'].unique().tolist()\n",
    "    completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    symbols_to_process = [s for s in all_symbols if s not in completed_symbols]\n",
    "\n",
    "    if not symbols_to_process:\n",
    "        print(\"All required symbols have already been processed. No API calls needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting API fetch for {len(symbols_to_process)} symbols.\")\n",
    "    \n",
    "    # --- Rate Limiting and Concurrency Control ---\n",
    "    # To stay under the 2500 requests/minute limit, we'll target 40 requests/second.\n",
    "    # This means we should start a new request no more frequently than every 1/40 = 0.025 seconds.\n",
    "    API_CALL_INTERVAL = 1.0 / 40.0\n",
    "    \n",
    "    # The semaphore controls how many requests can be active at once.\n",
    "    # This prevents overwhelming the server with connections and manages local resource usage.\n",
    "    CONCURRENCY_LIMIT = 15\n",
    "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        with tqdm(total=len(symbols_to_process), desc=\"Fetching Market Caps\") as pbar:\n",
    "            \n",
    "            async def fetch_with_sem(symbol):\n",
    "                # This wrapper function acquires the semaphore before making the API call.\n",
    "                async with sem:\n",
    "                    await fetch_market_cap_and_save(session, symbol, pbar, PARTIALS_DIR, API_KEY)\n",
    "                pbar.update(1)\n",
    "\n",
    "            tasks = []\n",
    "            for symbol in symbols_to_process:\n",
    "                # Create a task for each symbol...\n",
    "                task = asyncio.create_task(fetch_with_sem(symbol))\n",
    "                tasks.append(task)\n",
    "                # ...and then pause briefly before creating the next one.\n",
    "                await asyncio.sleep(API_CALL_INTERVAL)\n",
    "            \n",
    "            # Wait for all the created tasks to complete.\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"\\\\n--- API data fetching is complete. ---\")\n",
    "\n",
    "    # Check for any symbols that failed and output them\n",
    "    final_completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    failed_symbols = [s for s in all_symbols if s not in final_completed_symbols]\n",
    "\n",
    "    if failed_symbols:\n",
    "        failed_df = pd.DataFrame(failed_symbols, columns=['symbol'])\n",
    "        failed_df.to_csv(FAILED_SYMBOLS_PATH, index=False)\n",
    "        print(f\"Identified {len(failed_symbols)} symbols that could not be fetched. Their list has been saved to '{FAILED_SYMBOLS_PATH}'.\")\n",
    "    else:\n",
    "        print(\"All symbols were fetched successfully.\")\n",
    "\n",
    "# Run the API fetching process\n",
    "await fetch_market_cap_main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d138c6",
   "metadata": {},
   "source": [
    "## 3. Assemble Final Market Cap Reports\n",
    "\n",
    "Once the fetching script is complete, run this final cell. It will gather all the individual symbol files from the `partials` directory and assemble them into four final, alphabetized CSV files inside the `Market Cap Data` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a63b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data from 10532 individual symbol files...\n",
      "Saved chunk 'A-F' with 5206722 records to Market Cap Data\\market_cap_data_A-F.csv\n",
      "Saved chunk 'G-L' with 2494634 records to Market Cap Data\\market_cap_data_G-L.csv\n",
      "Saved chunk 'M-R' with 3223399 records to Market Cap Data\\market_cap_data_M-R.csv\n",
      "Saved chunk 'S-Z' with 3036302 records to Market Cap Data\\market_cap_data_S-Z.csv\n",
      "--- Assembly of final market cap reports is complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def assemble_market_cap_reports():\n",
    "    \"\"\"\n",
    "    Assembles individual symbol market cap CSVs from the 'partials' directory\n",
    "    into four final alphabetized batch files.\n",
    "    \"\"\"\n",
    "    partials_dir = 'Market Cap Data/partials'\n",
    "    output_dir = 'Market Cap Data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(partials_dir):\n",
    "        print(f\"Directory with partial files not found: {partials_dir}\")\n",
    "        return\n",
    "\n",
    "    all_files = glob.glob(os.path.join(partials_dir, '*.csv'))\n",
    "    if not all_files:\n",
    "        print(\"No partial symbol files were found to assemble.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Assembling data from {len(all_files)} individual symbol files...\")\n",
    "    \n",
    "    # Use a generator to save memory\n",
    "    df_generator = (pd.read_csv(f) for f in all_files)\n",
    "    full_df = pd.concat(df_generator, ignore_index=True)\n",
    "    \n",
    "    if 'symbol' not in full_df.columns:\n",
    "        print(\"Error: 'symbol' column not found in the assembled data. Cannot create batches.\")\n",
    "        return\n",
    "\n",
    "    # Ensure symbol column is string for sorting\n",
    "    full_df['symbol'] = full_df['symbol'].astype(str)\n",
    "\n",
    "    # Split into alphabetical chunks\n",
    "    symbol_chunks = {\n",
    "        \"A-F\": full_df[full_df['symbol'].str[0].str.upper() <= 'F'],\n",
    "        \"G-L\": full_df[(full_df['symbol'].str[0].str.upper() >= 'G') & (full_df['symbol'].str[0].str.upper() <= 'L')],\n",
    "        \"M-R\": full_df[(full_df['symbol'].str[0].str.upper() >= 'M') & (full_df['symbol'].str[0].str.upper() <= 'R')],\n",
    "        \"S-Z\": full_df[full_df['symbol'].str[0].str.upper() >= 'S']\n",
    "    }\n",
    "\n",
    "    for chunk_name, chunk_df in symbol_chunks.items():\n",
    "        if not chunk_df.empty:\n",
    "            output_path = os.path.join(output_dir, f\"market_cap_data_{chunk_name}.csv\")\n",
    "            chunk_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved chunk '{chunk_name}' with {len(chunk_df)} records to {output_path}\")\n",
    "        else:\n",
    "            print(f\"No data for chunk '{chunk_name}'.\")\n",
    "    \n",
    "    print(\"--- Assembly of final market cap reports is complete. ---\")\n",
    "\n",
    "# Run the assembly function\n",
    "assemble_market_cap_reports()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f084e",
   "metadata": {},
   "source": [
    "## 4. Fetch Historical Market Cap Data (10-40 Years Ago)\n",
    "\n",
    "This script fetches historical daily market cap data for the period from January 1, 1985, to 10 years before the present date. It uses the same robust, resumable, and concurrent process as the other fetching scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37629db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10-30 year API fetch for 10798 symbols (from 1985-01-01 to 2015-08-28).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce51c3ff8a3486099cfdc49c0e7f50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10-30yr Market Caps:   0%|          | 0/10798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- API data fetching for 10-30 year range is complete. ---\n",
      "Identified 6732 symbols that could not be fetched. List saved to 'symbols_with_no_market_cap_30yr.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_market_cap_and_save_30yr(session, symbol, pbar, partials_dir, api_key, from_date, to_date):\n",
    "    \"\"\"Fetches, processes, and saves daily market cap for a single symbol for the 10-30 year range.\"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/stable/historical-market-capitalization\"\n",
    "    params = {\"symbol\": symbol, \"from\": from_date, \"to\": to_date, \"apikey\": api_key}\n",
    "    \n",
    "    max_retries = 3\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=60) as response:\n",
    "                if response.status == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", 15))\n",
    "                    pbar.write(f\"Rate limit hit for {symbol}. Waiting {retry_after}s...\")\n",
    "                    await asyncio.sleep(retry_after)\n",
    "                    continue\n",
    "\n",
    "                response.raise_for_status()\n",
    "                data = await response.json()\n",
    "                \n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    if not df.empty:\n",
    "                        output_path = os.path.join(partials_dir, f\"{symbol}.csv\")\n",
    "                        df.to_csv(output_path, index=False)\n",
    "                    return # Success\n",
    "                else:\n",
    "                    return # No data found, but request was successful\n",
    "\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            if attempts < max_retries:\n",
    "                wait_time = 2 * attempts\n",
    "                pbar.write(f\"Request for {symbol} failed with {e}. Retrying in {wait_time}s... (Attempt {attempts}/{max_retries})\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                pbar.write(f\"API fetch failed for {symbol} after {max_retries} retries. Final error: {e}\")\n",
    "    return\n",
    "\n",
    "async def fetch_market_cap_main_30yr():\n",
    "    \"\"\"Main function to fetch 10-30 year market cap data.\"\"\"\n",
    "    # --- Configuration ---\n",
    "    SYMBOL_LIST_PATH = 'symbols_for_market_cap.csv'\n",
    "    PARTIALS_DIR = 'Market Cap Data/partials_30yr'\n",
    "    FAILED_SYMBOLS_PATH = 'symbols_with_no_market_cap_30yr.csv'\n",
    "    \n",
    "    os.makedirs(PARTIALS_DIR, exist_ok=True)\n",
    "    \n",
    "    load_dotenv()\n",
    "    API_KEY = os.getenv('API_KEY')\n",
    "    if not API_KEY:\n",
    "        print(\"API_KEY not found in .env file.\")\n",
    "        return\n",
    "\n",
    "    # 1. Define date range\n",
    "    from_date = '1985-01-01'\n",
    "    to_date = (datetime.now() - timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # 2. Load symbols and check against already completed ones\n",
    "    try:\n",
    "        symbols_to_fetch_df = pd.read_csv(SYMBOL_LIST_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"'{SYMBOL_LIST_PATH}' not found. Please run the symbol consolidation cell first.\")\n",
    "        return\n",
    "        \n",
    "    all_symbols = symbols_to_fetch_df['symbol'].unique().tolist()\n",
    "    completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    symbols_to_process = [s for s in all_symbols if s not in completed_symbols]\n",
    "\n",
    "    if not symbols_to_process:\n",
    "        print(\"All required symbols for the 10-30 year range have already been processed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting 10-30 year API fetch for {len(symbols_to_process)} symbols (from {from_date} to {to_date}).\")\n",
    "    \n",
    "    # 3. Asynchronous fetching with rate limiting\n",
    "    CONCURRENCY_LIMIT = 20\n",
    "    API_CALL_INTERVAL = 0.077 # To stay safely under 3000 calls/min\n",
    "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        with tqdm(total=len(symbols_to_process), desc=\"Fetching 10-30yr Market Caps\") as pbar:\n",
    "            tasks = []\n",
    "            \n",
    "            async def fetch_with_sem(symbol):\n",
    "                async with sem:\n",
    "                    await fetch_market_cap_and_save_30yr(session, symbol, pbar, PARTIALS_DIR, API_KEY, from_date, to_date)\n",
    "                pbar.update(1)\n",
    "\n",
    "            for symbol in symbols_to_process:\n",
    "                tasks.append(asyncio.create_task(fetch_with_sem(symbol)))\n",
    "                await asyncio.sleep(API_CALL_INTERVAL)\n",
    "            \n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"\\n--- API data fetching for 10-30 year range is complete. ---\")\n",
    "\n",
    "    # 4. Check for any symbols that failed and save them\n",
    "    final_completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    failed_symbols = [s for s in all_symbols if s not in final_completed_symbols]\n",
    "\n",
    "    if failed_symbols:\n",
    "        failed_df = pd.DataFrame(failed_symbols, columns=['symbol'])\n",
    "        failed_df.to_csv(FAILED_SYMBOLS_PATH, index=False)\n",
    "        print(f\"Identified {len(failed_symbols)} symbols that could not be fetched. List saved to '{FAILED_SYMBOLS_PATH}'.\")\n",
    "    else:\n",
    "        print(\"All symbols were fetched successfully.\")\n",
    "\n",
    "# Run the API fetching process\n",
    "await fetch_market_cap_main_30yr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f02da",
   "metadata": {},
   "source": [
    "## 5. Assemble 30-Year Price Data Reports\n",
    "\n",
    "This final cell assembles the individual symbol files from the `partials_30yr` directory into four alphabetized CSV files. The final files will be stored in the `Market Cap Data` directory with a `30yr_` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d96a3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling 30-year data from 4066 individual symbol files...\n",
      "Saved 30-year chunk 'A-F' with 4641300 records to Market Cap Data\\30yr_market_cap_data_A-F.csv\n",
      "Saved 30-year chunk 'G-L' with 2167286 records to Market Cap Data\\30yr_market_cap_data_G-L.csv\n",
      "Saved 30-year chunk 'M-R' with 2848765 records to Market Cap Data\\30yr_market_cap_data_M-R.csv\n",
      "Saved 30-year chunk 'S-Z' with 2669492 records to Market Cap Data\\30yr_market_cap_data_S-Z.csv\n",
      "--- Assembly of final 30-year market cap reports is complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def assemble_market_cap_reports_30yr():\n",
    "    \"\"\"\n",
    "    Assembles individual symbol market cap CSVs from the 'partials_30yr' directory\n",
    "    into four final alphabetized batch files.\n",
    "    \"\"\"\n",
    "    partials_dir = 'Market Cap Data/partials_30yr'\n",
    "    output_dir = 'Market Cap Data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(partials_dir):\n",
    "        print(f\"Directory with partial files not found: {partials_dir}\")\n",
    "        return\n",
    "\n",
    "    all_files = glob.glob(os.path.join(partials_dir, '*.csv'))\n",
    "    if not all_files:\n",
    "        print(\"No partial 30-year symbol files were found to assemble.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Assembling 30-year data from {len(all_files)} individual symbol files...\")\n",
    "    \n",
    "    # Use a generator to save memory\n",
    "    df_generator = (pd.read_csv(f) for f in all_files)\n",
    "    full_df = pd.concat(df_generator, ignore_index=True)\n",
    "    \n",
    "    if 'symbol' not in full_df.columns:\n",
    "        print(\"Error: 'symbol' column not found in the assembled data. Cannot create batches.\")\n",
    "        return\n",
    "\n",
    "    # Ensure symbol column is string for sorting\n",
    "    full_df['symbol'] = full_df['symbol'].astype(str)\n",
    "\n",
    "    # Split into alphabetical chunks\n",
    "    symbol_chunks = {\n",
    "        \"A-F\": full_df[full_df['symbol'].str[0].str.upper() <= 'F'],\n",
    "        \"G-L\": full_df[(full_df['symbol'].str[0].str.upper() >= 'G') & (full_df['symbol'].str[0].str.upper() <= 'L')],\n",
    "        \"M-R\": full_df[(full_df['symbol'].str[0].str.upper() >= 'M') & (full_df['symbol'].str[0].str.upper() <= 'R')],\n",
    "        \"S-Z\": full_df[full_df['symbol'].str[0].str.upper() >= 'S']\n",
    "    }\n",
    "\n",
    "    for chunk_name, chunk_df in symbol_chunks.items():\n",
    "        if not chunk_df.empty:\n",
    "            output_path = os.path.join(output_dir, f\"30yr_market_cap_data_{chunk_name}.csv\")\n",
    "            chunk_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved 30-year chunk '{chunk_name}' with {len(chunk_df)} records to {output_path}\")\n",
    "        else:\n",
    "            print(f\"No 30-year data for chunk '{chunk_name}'.\")\n",
    "    \n",
    "    print(\"--- Assembly of final 30-year market cap reports is complete. ---\")\n",
    "\n",
    "# Run the assembly function\n",
    "assemble_market_cap_reports_30yr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd118e",
   "metadata": {},
   "source": [
    "## 6. Consolidate All Market Cap Data\n",
    "\n",
    "This final step merges the 10-year and 30-year market cap data files. It reads each corresponding pair of chunked files (e.g., `market_cap_data_A-F.csv` and `30yr_market_cap_data_A-F.csv`), combines them, removes any duplicate rows based on the symbol and date, and overwrites the original 10-year file with the consolidated data. This leaves you with a single, complete set of market cap data spanning the last 30+ years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9de5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting consolidation of all market cap data. ---\n",
      "Processing chunk 'A-F'...\n",
      "  - Combined 'A-F': 9848022 rows -> 9843561 rows after deduplication.\n",
      "  - Saved consolidated data to 'Market Cap Data\\market_cap_data_A-F.csv'.\n",
      "Processing chunk 'G-L'...\n",
      "  - Combined 'G-L': 4661920 rows -> 4659782 rows after deduplication.\n",
      "  - Saved consolidated data to 'Market Cap Data\\market_cap_data_G-L.csv'.\n",
      "Processing chunk 'M-R'...\n",
      "  - Combined 'M-R': 6072164 rows -> 6069387 rows after deduplication.\n",
      "  - Saved consolidated data to 'Market Cap Data\\market_cap_data_M-R.csv'.\n",
      "Processing chunk 'S-Z'...\n",
      "  - Combined 'S-Z': 5705794 rows -> 5703161 rows after deduplication.\n",
      "  - Saved consolidated data to 'Market Cap Data\\market_cap_data_S-Z.csv'.\n",
      "--- Consolidation of all market cap reports is complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def consolidate_all_market_cap_data():\n",
    "    \"\"\"\n",
    "    Merges the 10-year and 30-year market cap data files, removing duplicates.\n",
    "    \"\"\"\n",
    "    data_dir = 'Market Cap Data'\n",
    "    chunks = [\"A-F\", \"G-L\", \"M-R\", \"S-Z\"]\n",
    "    \n",
    "    print(\"--- Starting consolidation of all market cap data. ---\")\n",
    "\n",
    "    for chunk in chunks:\n",
    "        ten_yr_file = os.path.join(data_dir, f\"market_cap_data_{chunk}.csv\")\n",
    "        thirty_yr_file = os.path.join(data_dir, f\"30yr_market_cap_data_{chunk}.csv\")\n",
    "\n",
    "        # Check if both files exist before trying to merge\n",
    "        if not os.path.exists(ten_yr_file):\n",
    "            print(f\"Warning: Base file not found for chunk '{chunk}'. Skipping.\")\n",
    "            continue\n",
    "        if not os.path.exists(thirty_yr_file):\n",
    "            print(f\"Warning: 30-year file not found for chunk '{chunk}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing chunk '{chunk}'...\")\n",
    "\n",
    "        # Read both files\n",
    "        ten_yr_df = pd.read_csv(ten_yr_file)\n",
    "        thirty_yr_df = pd.read_csv(thirty_yr_file)\n",
    "        \n",
    "        # Concatenate, drop duplicates, and save\n",
    "        combined_df = pd.concat([ten_yr_df, thirty_yr_df], ignore_index=True)\n",
    "        \n",
    "        initial_rows = len(combined_df)\n",
    "        combined_df.drop_duplicates(subset=['symbol', 'date'], keep='first', inplace=True)\n",
    "        final_rows = len(combined_df)\n",
    "        \n",
    "        # Overwrite the original 10-year file with the fully consolidated data\n",
    "        combined_df.to_csv(ten_yr_file, index=False)\n",
    "        \n",
    "        print(f\"  - Combined '{chunk}': {initial_rows} rows -> {final_rows} rows after deduplication.\")\n",
    "        print(f\"  - Saved consolidated data to '{ten_yr_file}'.\")\n",
    "\n",
    "    print(\"--- Consolidation of all market cap reports is complete. ---\")\n",
    "\n",
    "# Run the consolidation function\n",
    "consolidate_all_market_cap_data()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
