{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada4499d",
   "metadata": {},
   "source": [
    "## 1. Consolidate Symbols for Price Fetching\n",
    "\n",
    "First, we need to create a master list of all unique symbols from the financial statement files we generated in the previous notebook. This will be our target list for fetching historical price data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab46b80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 financial statement files to process.\n",
      "Found 298487 total financial records.\n",
      "Extracted 10995 unique symbols.\n",
      "Master symbol list saved to 'symbols_from_statements.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def consolidate_symbols_from_financials():\n",
    "    \"\"\"\n",
    "    Reads all 'financials_*.csv' files, gets a unique list of symbols,\n",
    "    and saves it to a new CSV file.\n",
    "    \"\"\"\n",
    "    financials_dir = '../Statements/Statement Data'\n",
    "    output_path = 'symbols_from_statements.csv'\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.isdir(financials_dir):\n",
    "        print(f\"Error: Directory not found at '{financials_dir}'. Please ensure financial statements have been generated.\")\n",
    "        return\n",
    "\n",
    "    # Find all financial statement CSV files\n",
    "    csv_files = glob.glob(os.path.join(financials_dir, 'financials_*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No financial statement CSVs found in '{financials_dir}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(csv_files)} financial statement files to process.\")\n",
    "    \n",
    "    # Read and concatenate all files\n",
    "    df_list = [pd.read_csv(f) for f in csv_files]\n",
    "    full_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Get unique symbols\n",
    "    unique_symbols = full_df['symbol'].unique()\n",
    "    \n",
    "    # Create a new DataFrame and save to CSV\n",
    "    symbols_df = pd.DataFrame(unique_symbols, columns=['symbol'])\n",
    "    symbols_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Found {len(full_df)} total financial records.\")\n",
    "    print(f\"Extracted {len(symbols_df)} unique symbols.\")\n",
    "    print(f\"Master symbol list saved to '{output_path}'.\")\n",
    "\n",
    "# Run the function\n",
    "consolidate_symbols_from_financials()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fadf0a5",
   "metadata": {},
   "source": [
    "## 2. Phase 1: Process Data from Local 9.5GB CSV\n",
    "\n",
    "The first part of our hybrid approach is to leverage your large local data file. This script will use DuckDB to query the `daily_price_data.csv` without loading the whole file into memory.\n",
    "\n",
    "For every symbol found, it will extract the last trading day of each month for the past 10 years and save it as a separate file in the `partials_price` directory. Any symbols **not** found in the local file will be saved to a separate list to be handled in the next phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ab20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 symbols already processed. Checking for 10995 remaining symbols in local CSV.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce1600334834e4aa5cb9cce331a3972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving daily data from local CSV:   0%|          | 0/8818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed and saved data for 8818 symbols from the local file.\n",
      "\n",
      "Saved a list of 2177 symbols to be fetched from the API to '../output/symbols_to_fetch_from_api.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def process_local_csv_for_prices():\n",
    "    \"\"\"\n",
    "    Uses DuckDB to query the large local CSV for daily price data from the last 10 years,\n",
    "    saves the results to partial files, and creates a list of symbols\n",
    "    that were not found for the next phase.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    SYMBOLS_FILE_PATH = 'symbols_from_statements.csv'\n",
    "    BIG_CSV_PATH = 'Price Data/daily_price_data.csv'\n",
    "    PARTIALS_DIR = 'Price Data/partials_price'\n",
    "    API_FETCH_LIST_PATH = 'symbols_not_found_locally.csv'\n",
    "    \n",
    "    os.makedirs(PARTIALS_DIR, exist_ok=True)\n",
    "\n",
    "    # 1. Load all symbols and determine which ones still need to be processed\n",
    "    all_symbols_df = pd.read_csv(SYMBOLS_FILE_PATH)\n",
    "    all_symbols = all_symbols_df['symbol'].unique().tolist()\n",
    "    completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    symbols_to_process = [s for s in all_symbols if s not in completed_symbols]\n",
    "    \n",
    "    print(f\"{len(completed_symbols)} symbols already processed. Checking for {len(symbols_to_process)} remaining symbols in local CSV.\")\n",
    "    \n",
    "    symbols_found_in_csv = []\n",
    "    \n",
    "    # 2. Query the large CSV if it exists\n",
    "    if os.path.exists(BIG_CSV_PATH):\n",
    "        if not symbols_to_process:\n",
    "            print(\"All required symbols have already been processed.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Connect to DuckDB\n",
    "                con = duckdb.connect()\n",
    "                con.execute(\"PRAGMA disable_progress_bar;\")\n",
    "\n",
    "                # Define date range for the query\n",
    "                ten_years_ago_str = (datetime.now() - timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "\n",
    "                # Optimized query for daily data within the last 10 years\n",
    "                query = f\"\"\"\n",
    "                SELECT\n",
    "                    \"date\",\n",
    "                    \"symbol\",\n",
    "                    \"adjClose\"\n",
    "                FROM read_csv_auto('{BIG_CSV_PATH}')\n",
    "                WHERE \"symbol\" IN {tuple(symbols_to_process)} AND \"date\" >= '{ten_years_ago_str}';\n",
    "                \"\"\"\n",
    "                results_df = con.execute(query).df()\n",
    "                con.close()\n",
    "                \n",
    "                if not results_df.empty:\n",
    "                    grouped = results_df.groupby('symbol')\n",
    "                    for symbol, group_df in tqdm(grouped, desc=\"Saving daily data from local CSV\"):\n",
    "                        output_path = os.path.join(PARTIALS_DIR, f\"{symbol}.csv\")\n",
    "                        group_df.to_csv(output_path, index=False)\n",
    "                        symbols_found_in_csv.append(symbol)\n",
    "                    print(f\"\\nSuccessfully processed and saved data for {len(symbols_found_in_csv)} symbols from the local file.\")\n",
    "                else:\n",
    "                    print(\"No new symbols found in the local CSV for the specified date range.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"DuckDB query failed: {e}\")\n",
    "                if 'con' in locals():\n",
    "                    con.close()\n",
    "    else:\n",
    "        print(f\"Large CSV file not found at '{BIG_CSV_PATH}'. All symbols will be fetched from the API.\")\n",
    "\n",
    "    # 3. Create the list of symbols that still need to be fetched from the API\n",
    "    symbols_for_api = [s for s in symbols_to_process if s not in symbols_found_in_csv]\n",
    "    \n",
    "    if symbols_for_api:\n",
    "        api_df = pd.DataFrame(symbols_for_api, columns=['symbol'])\n",
    "        api_df.to_csv(API_FETCH_LIST_PATH, index=False)\n",
    "        print(f\"\\nSaved a list of {len(symbols_for_api)} symbols to be fetched from the API to '{API_FETCH_LIST_PATH}'.\")\n",
    "    else:\n",
    "        print(\"\\nNo symbols need to be fetched from the API. All required data was found locally.\")\n",
    "        # Create an empty file to prevent errors in the next step\n",
    "        pd.DataFrame(columns=['symbol']).to_csv(API_FETCH_LIST_PATH, index=False)\n",
    "\n",
    "# Run the local processing function\n",
    "process_local_csv_for_prices()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24ef78",
   "metadata": {},
   "source": [
    "## 2. Phase 2: Fetch Remaining Data from API\n",
    "\n",
    "Now, run this cell to fetch the price data for all the symbols that were **not** found in the local CSV file.\n",
    "\n",
    "It reads the list of missing symbols from `symbols_not_found_locally.csv` and uses the same robust, resumable, and concurrent fetching logic as our previous tasks. It will save each successfully fetched symbol to the same `partials_price` directory, and you can stop and restart it without losing progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d49fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API fetch for 2177 symbols.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff33a392e8c443cb63fcaead9a1a51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching from API:   0%|          | 0/2177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- API data fetching is complete. ---\n",
      "Identified 196 symbols that could not be fetched. Their list has been saved to '../output/symbols_with_no_price.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_api_data_and_save(session, symbol, pbar, partials_dir, api_key):\n",
    "    \"\"\"Fetches, processes, and saves daily data for a single symbol from the API.\"\"\"\n",
    "    ten_years_ago = (datetime.now() - timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f\"https://financialmodelingprep.com/stable/historical-price-eod/dividend-adjusted\"\n",
    "    params = {\"symbol\": symbol, \"from\": ten_years_ago, \"to\": today, \"apikey\": api_key}\n",
    "    \n",
    "    max_retries = 3\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=60) as response:\n",
    "                if response.status == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", 15)) # Default to 15s if header is missing\n",
    "                    pbar.write(f\"Rate limit hit for {symbol}. Waiting {retry_after}s...\")\n",
    "                    await asyncio.sleep(retry_after)\n",
    "                    continue # Retry without incrementing attempts\n",
    "\n",
    "                response.raise_for_status()\n",
    "                data = await response.json()\n",
    "                # Corrected logic: The response is a list directly, not nested under 'historical'\n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    if not df.empty:\n",
    "                        # Ensure 'symbol' column exists for consistency, as it might be missing in some API responses\n",
    "                        if 'symbol' not in df.columns:\n",
    "                            df['symbol'] = symbol\n",
    "                        output_path = os.path.join(partials_dir, f\"{symbol}.csv\")\n",
    "                        df_to_save = df[['date', 'symbol', 'adjClose']]\n",
    "                        df_to_save.to_csv(output_path, index=False)\n",
    "                    return # Success\n",
    "                else:\n",
    "                    # No data found, but request was successful. Count as done.\n",
    "                    return\n",
    "\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            if attempts < max_retries:\n",
    "                wait_time = 2 * attempts # Exponential backoff\n",
    "                pbar.write(f\"Request for {symbol} failed with {e}. Retrying in {wait_time}s... (Attempt {attempts}/{max_retries})\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                pbar.write(f\"API fetch failed for {symbol} after {max_retries} retries. Final error: {e}\")\n",
    "    return\n",
    "\n",
    "async def fetch_api_data_main():\n",
    "    \"\"\"Main function to fetch API data for symbols listed in the 'to_fetch' file.\"\"\"\n",
    "    # --- Configuration ---\n",
    "    API_FETCH_LIST_PATH = 'symbols_not_found_locally.csv'\n",
    "    PARTIALS_DIR = 'Price Data/partials_price'\n",
    "    FAILED_SYMBOLS_PATH = 'symbols_with_no_price.csv'\n",
    "    \n",
    "    load_dotenv()\n",
    "    API_KEY = os.getenv('API_KEY')\n",
    "    if not API_KEY:\n",
    "        print(\"API_KEY not found in .env file.\")\n",
    "        return\n",
    "\n",
    "    # 1. Load symbols to fetch and check against already completed ones\n",
    "    try:\n",
    "        symbols_to_fetch_df = pd.read_csv(API_FETCH_LIST_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"'{API_FETCH_LIST_PATH}' not found. Please run the local CSV processing cell first.\")\n",
    "        return\n",
    "        \n",
    "    all_api_symbols = symbols_to_fetch_df['symbol'].unique().tolist()\n",
    "    completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    symbols_to_process = [s for s in all_api_symbols if s not in completed_symbols]\n",
    "\n",
    "    if not symbols_to_process:\n",
    "        print(\"All required symbols have already been processed. No API calls needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting API fetch for {len(symbols_to_process)} symbols.\")\n",
    "    \n",
    "    # 2. Asynchronous fetching with reduced concurrency\n",
    "    sem = asyncio.Semaphore(15)\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        with tqdm(total=len(symbols_to_process), desc=\"Fetching from API\") as pbar:\n",
    "            tasks = []\n",
    "            for symbol in symbols_to_process:\n",
    "                async def fetch_task(s):\n",
    "                    async with sem:\n",
    "                        await fetch_api_data_and_save(session, s, pbar, PARTIALS_DIR, API_KEY)\n",
    "                    pbar.update(1)\n",
    "                tasks.append(fetch_task(symbol))\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"\\n--- API data fetching is complete. ---\")\n",
    "\n",
    "    # 3. Check for any symbols that failed during this run and output them\n",
    "    final_completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    failed_symbols = [s for s in all_api_symbols if s not in final_completed_symbols]\n",
    "\n",
    "    if failed_symbols:\n",
    "        failed_df = pd.DataFrame(failed_symbols, columns=['symbol'])\n",
    "        failed_df.to_csv(FAILED_SYMBOLS_PATH, index=False)\n",
    "        print(f\"Identified {len(failed_symbols)} symbols that could not be fetched. Their list has been saved to '{FAILED_SYMBOLS_PATH}'.\")\n",
    "    else:\n",
    "        print(\"All symbols were fetched successfully.\")\n",
    "\n",
    "# Run the API fetching process\n",
    "await fetch_api_data_main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138e1a8",
   "metadata": {},
   "source": [
    "## 3. Assemble Final Price Data Reports\n",
    "\n",
    "Once the fetching script above is complete, run this final cell. It will gather all the individual symbol files from the `partials_price` directory and assemble them into four final, alphabetized CSV files, which are easier to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4f50ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling data from 10801 individual symbol files...\n",
      "Saved chunk 'A-F' with 5243452 records to Price Data\\price_data_A-F.csv\n",
      "Saved chunk 'G-L' with 2502525 records to Price Data\\price_data_G-L.csv\n",
      "Saved chunk 'M-R' with 3249138 records to Price Data\\price_data_M-R.csv\n",
      "Saved chunk 'S-Z' with 3052851 records to Price Data\\price_data_S-Z.csv\n",
      "--- Assembly of final price reports is complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def assemble_price_reports():\n",
    "    \"\"\"\n",
    "    Assembles individual symbol price CSVs from the 'partials_price' directory\n",
    "    into four final alphabetized batch files.\n",
    "    \"\"\"\n",
    "    partials_dir = 'Price Data/partials_price'\n",
    "    output_dir = 'Price Data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(partials_dir):\n",
    "        print(f\"Directory with partial files not found: {partials_dir}\")\n",
    "        return\n",
    "\n",
    "    all_files = glob.glob(os.path.join(partials_dir, '*.csv'))\n",
    "    if not all_files:\n",
    "        print(\"No partial symbol files were found to assemble.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Assembling data from {len(all_files)} individual symbol files...\")\n",
    "    \n",
    "    # Use a generator to save memory\n",
    "    df_generator = (pd.read_csv(f) for f in all_files)\n",
    "    full_df = pd.concat(df_generator, ignore_index=True)\n",
    "    \n",
    "    if 'symbol' not in full_df.columns:\n",
    "        print(\"Error: 'symbol' column not found in the assembled data. Cannot create batches.\")\n",
    "        return\n",
    "\n",
    "    # Split into alphabetical chunks\n",
    "    symbol_chunks = {\n",
    "        \"A-F\": full_df[full_df['symbol'].str[0].str.upper() <= 'F'],\n",
    "        \"G-L\": full_df[(full_df['symbol'].str[0].str.upper() >= 'G') & (full_df['symbol'].str[0].str.upper() <= 'L')],\n",
    "        \"M-R\": full_df[(full_df['symbol'].str[0].str.upper() >= 'M') & (full_df['symbol'].str[0].str.upper() <= 'R')],\n",
    "        \"S-Z\": full_df[full_df['symbol'].str[0].str.upper() >= 'S']\n",
    "    }\n",
    "\n",
    "    for chunk_name, chunk_df in symbol_chunks.items():\n",
    "        if not chunk_df.empty:\n",
    "            output_path = os.path.join(output_dir, f\"price_data_{chunk_name}.csv\")\n",
    "            chunk_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved chunk '{chunk_name}' with {len(chunk_df)} records to {output_path}\")\n",
    "        else:\n",
    "            print(f\"No data for chunk '{chunk_name}'.\")\n",
    "    \n",
    "    print(\"--- Assembly of final price reports is complete. ---\")\n",
    "\n",
    "# Run the assembly function\n",
    "assemble_price_reports()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c4884d",
   "metadata": {},
   "source": [
    "## 4. Fetch Historical Price Data (10-40 Years Ago)\n",
    "\n",
    "This section fetches historical price data for the period starting from January 1, 1985, up to 10 years before the current date. This complements the 10-year data already fetched.\n",
    "\n",
    "The process is designed to be efficient and resumable:\n",
    "- It uses an asynchronous approach with `aiohttp` to handle many requests concurrently.\n",
    "- A semaphore limits concurrency to avoid overwhelming the API server.\n",
    "- A delay is introduced between creating fetch tasks to respect the API rate limit (3000 calls/minute).\n",
    "- Progress is saved to a `partials_price_30yr` directory, so if the script is interrupted, it can be resumed without losing work.\n",
    "- Any symbols that fail to fetch after multiple retries are logged to `symbols_with_no_price_30yr.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0cf1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10-30 year API fetch for 10995 symbols (from 1985-01-01 to 2015-08-28).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e5fd60389043fba56dae0244b8eab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10-30yr Prices:   0%|          | 0/10995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- API data fetching for 10-30 year range is complete. ---\n",
      "Identified 6895 symbols that could not be fetched. List saved to 'symbols_with_no_price_30yr.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_api_data_and_save_30yr(session, symbol, pbar, partials_dir, api_key, from_date, to_date):\n",
    "    \"\"\"Fetches, processes, and saves daily data for a single symbol from the API for the 10-30 year range.\"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/stable/historical-price-eod/dividend-adjusted\"\n",
    "    params = {\"symbol\": symbol, \"from\": from_date, \"to\": to_date, \"apikey\": api_key}\n",
    "    \n",
    "    max_retries = 3\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=60) as response:\n",
    "                if response.status == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", 15))\n",
    "                    pbar.write(f\"Rate limit hit for {symbol}. Waiting {retry_after}s...\")\n",
    "                    await asyncio.sleep(retry_after)\n",
    "                    continue\n",
    "\n",
    "                response.raise_for_status()\n",
    "                data = await response.json()\n",
    "                \n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    if not df.empty:\n",
    "                        if 'symbol' not in df.columns:\n",
    "                            df['symbol'] = symbol\n",
    "                        output_path = os.path.join(partials_dir, f\"{symbol}.csv\")\n",
    "                        df_to_save = df[['date', 'symbol', 'adjClose']]\n",
    "                        df_to_save.to_csv(output_path, index=False)\n",
    "                    return # Success\n",
    "                else:\n",
    "                    return # No data found, but request was successful\n",
    "\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            if attempts < max_retries:\n",
    "                wait_time = 2 * attempts\n",
    "                pbar.write(f\"Request for {symbol} failed with {e}. Retrying in {wait_time}s... (Attempt {attempts}/{max_retries})\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                pbar.write(f\"API fetch failed for {symbol} after {max_retries} retries. Final error: {e}\")\n",
    "    return\n",
    "\n",
    "async def fetch_api_data_main_30yr():\n",
    "    \"\"\"Main function to fetch historical API data for all symbols.\"\"\"\n",
    "    # --- Configuration ---\n",
    "    SYMBOLS_FILE_PATH = 'symbols_from_statements.csv'\n",
    "    PARTIALS_DIR = 'Price Data/partials_price_30yr'\n",
    "    FAILED_SYMBOLS_PATH = 'symbols_with_no_price_30yr.csv'\n",
    "    \n",
    "    os.makedirs(PARTIALS_DIR, exist_ok=True)\n",
    "    \n",
    "    load_dotenv()\n",
    "    API_KEY = os.getenv('API_KEY')\n",
    "    if not API_KEY:\n",
    "        print(\"API_KEY not found in .env file.\")\n",
    "        return\n",
    "\n",
    "    # 1. Define date range\n",
    "    from_date = '1985-01-01'\n",
    "    to_date = (datetime.now() - timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # 2. Load symbols and check against already completed ones\n",
    "    try:\n",
    "        all_symbols_df = pd.read_csv(SYMBOLS_FILE_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"'{SYMBOLS_FILE_PATH}' not found. Please run the first cell to generate it.\")\n",
    "        return\n",
    "        \n",
    "    all_symbols = all_symbols_df['symbol'].unique().tolist()\n",
    "    completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    symbols_to_process = [s for s in all_symbols if s not in completed_symbols]\n",
    "\n",
    "    if not symbols_to_process:\n",
    "        print(\"All required symbols for the 10-30 year range have already been processed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting 10-30 year API fetch for {len(symbols_to_process)} symbols (from {from_date} to {to_date}).\")\n",
    "    \n",
    "    # 3. Asynchronous fetching with rate limiting\n",
    "    CONCURRENCY_LIMIT = 20\n",
    "    API_CALL_INTERVAL = 0.077 # To stay safely under 3000 calls/min\n",
    "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        with tqdm(total=len(symbols_to_process), desc=\"Fetching 10-30yr Prices\") as pbar:\n",
    "            tasks = []\n",
    "            \n",
    "            async def fetch_with_sem(symbol):\n",
    "                async with sem:\n",
    "                    await fetch_api_data_and_save_30yr(session, symbol, pbar, PARTIALS_DIR, API_KEY, from_date, to_date)\n",
    "                pbar.update(1)\n",
    "\n",
    "            for symbol in symbols_to_process:\n",
    "                tasks.append(asyncio.create_task(fetch_with_sem(symbol)))\n",
    "                await asyncio.sleep(API_CALL_INTERVAL)\n",
    "            \n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"\\n--- API data fetching for 10-30 year range is complete. ---\")\n",
    "\n",
    "    # 4. Check for any symbols that failed and save them\n",
    "    final_completed_symbols = {f.split('.')[0] for f in os.listdir(PARTIALS_DIR)}\n",
    "    failed_symbols = [s for s in all_symbols if s not in final_completed_symbols]\n",
    "\n",
    "    if failed_symbols:\n",
    "        failed_df = pd.DataFrame(failed_symbols, columns=['symbol'])\n",
    "        failed_df.to_csv(FAILED_SYMBOLS_PATH, index=False)\n",
    "        print(f\"Identified {len(failed_symbols)} symbols that could not be fetched. List saved to '{FAILED_SYMBOLS_PATH}'.\")\n",
    "    else:\n",
    "        print(\"All symbols were fetched successfully.\")\n",
    "\n",
    "# Run the API fetching process\n",
    "await fetch_api_data_main_30yr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1ca57",
   "metadata": {},
   "source": [
    "## 5. Assemble 30-Year Price Data Reports\n",
    "\n",
    "This final cell assembles the individual symbol files from the `partials_price_30yr` directory into four alphabetized CSV files. The final files will be stored in the `Price Data` directory with a `30yr_` prefix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba7e124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling 30-year data from 4100 individual symbol files...\n",
      "Saved 30-year chunk 'A-F' with 5563601 records to Price Data\\30yr_price_data_A-F.csv\n",
      "Saved 30-year chunk 'G-L' with 2594242 records to Price Data\\30yr_price_data_G-L.csv\n",
      "Saved 30-year chunk 'M-R' with 3411769 records to Price Data\\30yr_price_data_M-R.csv\n",
      "Saved 30-year chunk 'S-Z' with 3248351 records to Price Data\\30yr_price_data_S-Z.csv\n",
      "--- Assembly of final 30-year price reports is complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def assemble_price_reports_30yr():\n",
    "    \"\"\"\n",
    "    Assembles individual symbol price CSVs from the 'partials_price_30yr' directory\n",
    "    into four final alphabetized batch files.\n",
    "    \"\"\"\n",
    "    partials_dir = 'Price Data/partials_price_30yr'\n",
    "    output_dir = 'Price Data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(partials_dir):\n",
    "        print(f\"Directory with partial files not found: {partials_dir}\")\n",
    "        return\n",
    "\n",
    "    all_files = glob.glob(os.path.join(partials_dir, '*.csv'))\n",
    "    if not all_files:\n",
    "        print(\"No partial 30-year symbol files were found to assemble.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Assembling 30-year data from {len(all_files)} individual symbol files...\")\n",
    "    \n",
    "    df_generator = (pd.read_csv(f) for f in all_files)\n",
    "    full_df = pd.concat(df_generator, ignore_index=True)\n",
    "    \n",
    "    if 'symbol' not in full_df.columns:\n",
    "        print(\"Error: 'symbol' column not found in the assembled data. Cannot create batches.\")\n",
    "        return\n",
    "\n",
    "    # Split into alphabetical chunks\n",
    "    symbol_chunks = {\n",
    "        \"A-F\": full_df[full_df['symbol'].str[0].str.upper() <= 'F'],\n",
    "        \"G-L\": full_df[(full_df['symbol'].str[0].str.upper() >= 'G') & (full_df['symbol'].str[0].str.upper() <= 'L')],\n",
    "        \"M-R\": full_df[(full_df['symbol'].str[0].str.upper() >= 'M') & (full_df['symbol'].str[0].str.upper() <= 'R')],\n",
    "        \"S-Z\": full_df[full_df['symbol'].str[0].str.upper() >= 'S']\n",
    "    }\n",
    "\n",
    "    for chunk_name, chunk_df in symbol_chunks.items():\n",
    "        if not chunk_df.empty:\n",
    "            output_path = os.path.join(output_dir, f\"30yr_price_data_{chunk_name}.csv\")\n",
    "            chunk_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved 30-year chunk '{chunk_name}' with {len(chunk_df)} records to {output_path}\")\n",
    "        else:\n",
    "            print(f\"No 30-year data for chunk '{chunk_name}'.\")\n",
    "    \n",
    "    print(\"--- Assembly of final 30-year price reports is complete. ---\")\n",
    "\n",
    "# Run the assembly function\n",
    "assemble_price_reports_30yr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98764899",
   "metadata": {},
   "source": [
    "## 6. Consolidate All Price Data\n",
    "\n",
    "This final step merges the 10-year and 30-year price data files. It reads each corresponding pair of chunked files (e.g., `price_data_A-F.csv` and `30yr_price_data_A-F.csv`), combines them, removes any duplicate rows, and overwrites the original 10-year file with the consolidated data. This leaves you with a single, complete set of price data spanning the last 30+ years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f739b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting consolidation of all price data. ---\n",
      "Processing chunk 'A-F'...\n",
      "  - Combined 'A-F': 10807053 rows -> 10802555 rows after deduplication.\n",
      "  - Saved consolidated data to 'Price Data\\price_data_A-F.csv'.\n",
      "Processing chunk 'G-L'...\n",
      "  - Combined 'G-L': 5096767 rows -> 5094623 rows after deduplication.\n",
      "  - Saved consolidated data to 'Price Data\\price_data_G-L.csv'.\n",
      "Processing chunk 'M-R'...\n",
      "  - Combined 'M-R': 6660907 rows -> 6658106 rows after deduplication.\n",
      "  - Saved consolidated data to 'Price Data\\price_data_M-R.csv'.\n",
      "Processing chunk 'S-Z'...\n",
      "  - Combined 'S-Z': 6301202 rows -> 6298545 rows after deduplication.\n",
      "  - Saved consolidated data to 'Price Data\\price_data_S-Z.csv'.\n",
      "--- Consolidation of all price reports is complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def consolidate_all_price_data():\n",
    "    \"\"\"\n",
    "    Merges the 10-year and 30-year price data files, removing duplicates.\n",
    "    \"\"\"\n",
    "    data_dir = 'Price Data'\n",
    "    chunks = [\"A-F\", \"G-L\", \"M-R\", \"S-Z\"]\n",
    "    \n",
    "    print(\"--- Starting consolidation of all price data. ---\")\n",
    "\n",
    "    for chunk in chunks:\n",
    "        ten_yr_file = os.path.join(data_dir, f\"price_data_{chunk}.csv\")\n",
    "        thirty_yr_file = os.path.join(data_dir, f\"30yr_price_data_{chunk}.csv\")\n",
    "\n",
    "        if not os.path.exists(ten_yr_file):\n",
    "            print(f\"Warning: Base file not found for chunk '{chunk}'. Skipping.\")\n",
    "            continue\n",
    "        if not os.path.exists(thirty_yr_file):\n",
    "            print(f\"Warning: 30-year file not found for chunk '{chunk}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing chunk '{chunk}'...\")\n",
    "\n",
    "        # Read both files\n",
    "        ten_yr_df = pd.read_csv(ten_yr_file)\n",
    "        thirty_yr_df = pd.read_csv(thirty_yr_file)\n",
    "        \n",
    "        # Concatenate, drop duplicates, and save\n",
    "        combined_df = pd.concat([ten_yr_df, thirty_yr_df], ignore_index=True)\n",
    "        \n",
    "        initial_rows = len(combined_df)\n",
    "        combined_df.drop_duplicates(subset=['symbol', 'date'], keep='first', inplace=True)\n",
    "        final_rows = len(combined_df)\n",
    "        \n",
    "        # Overwrite the original 10-year file with the fully consolidated data\n",
    "        combined_df.to_csv(ten_yr_file, index=False)\n",
    "        \n",
    "        print(f\"  - Combined '{chunk}': {initial_rows} rows -> {final_rows} rows after deduplication.\")\n",
    "        print(f\"  - Saved consolidated data to '{ten_yr_file}'.\")\n",
    "\n",
    "    print(\"--- Consolidation of all price reports is complete. ---\")\n",
    "\n",
    "# Run the consolidation function\n",
    "consolidate_all_price_data()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
