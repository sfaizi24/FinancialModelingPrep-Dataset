{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebab1c6",
   "metadata": {},
   "source": [
    "## Fetching Financial Statements (Robust & Resumable)\n",
    "\n",
    "This next step will fetch 10 years of quarterly financial data for every symbol in our `fetchable_symbols.csv` list. This is a long-running process, but it has been redesigned to be **robust and resumable**.\n",
    "\n",
    "Hereâ€™s how it works:\n",
    "1.  **Pre-Flight Check:** It first runs a quick test on a single symbol to ensure the API is responding correctly.\n",
    "2.  **Individual Symbol Processing:** It fetches, processes, and saves the data for one symbol at a time to a temporary `partials` folder.\n",
    "3.  **Automatic Resuming:** If the script is stopped and restarted, it will automatically skip any symbols it has already completed, picking up where it left off.\n",
    "4.  **Clear Error Logging:** If a symbol fails repeatedly, it will now print a clear error message instead of failing silently.\n",
    "\n",
    "After this cell completes, a final script in the next cell will assemble these individual files into the four alphabetized batch files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_endpoint(session, url, params, symbol_for_logging):\n",
    "    \"\"\"Asynchronously fetches data from a single API endpoint with retries and logging.\"\"\"\n",
    "    retries = 3\n",
    "    last_error = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=30) as response:\n",
    "                if response.status == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", 5))\n",
    "                    await asyncio.sleep(retry_after)\n",
    "                    continue\n",
    "                response.raise_for_status()\n",
    "                return await response.json()\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            if i < retries - 1:\n",
    "                await asyncio.sleep((i + 1) * 2)\n",
    "            else:\n",
    "                endpoint_name = url.split('/')[-1]\n",
    "                tqdm.write(f\"ERROR for {symbol_for_logging}: Failed to fetch {endpoint_name} data after {retries} retries. Final error: {last_error}\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "async def fetch_and_process_symbol(session, symbol, api_key, partials_dir):\n",
    "    \"\"\"Fetches, merges, and saves data for a single symbol.\"\"\"\n",
    "    base_url = \"https://financialmodelingprep.com/stable\"\n",
    "    base_params = {\"period\": \"quarter\", \"limit\": 40, \"apikey\": api_key, \"symbol\": symbol}\n",
    "    \n",
    "    income_url = f\"{base_url}/income-statement\"\n",
    "    balance_url = f\"{base_url}/balance-sheet-statement\"\n",
    "    ratios_url = f\"{base_url}/ratios\"\n",
    "    \n",
    "    tasks = [\n",
    "        fetch_endpoint(session, income_url, base_params, symbol),\n",
    "        fetch_endpoint(session, balance_url, base_params, symbol),\n",
    "        fetch_endpoint(session, ratios_url, base_params, symbol)\n",
    "    ]\n",
    "    income_data, balance_data, ratios_data = await asyncio.gather(*tasks)\n",
    "\n",
    "    if not income_data and not balance_data and not ratios_data:\n",
    "        return\n",
    "\n",
    "    income_df = pd.DataFrame(income_data)\n",
    "    balance_df = pd.DataFrame(balance_data)\n",
    "    ratios_df = pd.DataFrame(ratios_data)\n",
    "    \n",
    "    all_dfs = []\n",
    "    if not income_df.empty: all_dfs.append(income_df.set_index(['date', 'symbol', 'period']))\n",
    "    if not balance_df.empty: all_dfs.append(balance_df.set_index(['date', 'symbol', 'period']))\n",
    "    if not ratios_df.empty: all_dfs.append(ratios_df.set_index(['date', 'symbol', 'period']))\n",
    "\n",
    "    if not all_dfs:\n",
    "        return\n",
    "\n",
    "    merged_df = pd.concat(all_dfs, axis=1).reset_index()\n",
    "    \n",
    "    columns_to_keep = {\n",
    "        'date': 'exactDate', 'symbol': 'symbol', 'revenue': 'revenue', 'netIncome': 'netIncome',\n",
    "        'ebitda': 'ebitda', 'eps': 'eps', 'priceToEarningsRatio': 'pToE', 'totalCurrentAssets': 'totalCurrentAssets',\n",
    "        'totalAssets': 'totalAssets', 'totalLiabilities': 'totalLiabilities', 'totalDebt': 'totalDebt',\n",
    "        'debtToAssetsRatio': 'debtToAssets', 'priceToBookRatio': 'pToB', 'debtToEquityRatio': 'debtToEquity',\n",
    "        'debtToCapitalRatio': 'debtToCapital', 'revenuePerShare': 'revenuePerShare'\n",
    "    }\n",
    "    \n",
    "    # Filter for columns that actually exist and drop duplicates from the join\n",
    "    merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "    existing_columns = {k: v for k, v in columns_to_keep.items() if k in merged_df.columns}\n",
    "    \n",
    "    if not existing_columns:\n",
    "        return\n",
    "\n",
    "    final_df = merged_df[list(existing_columns.keys())].rename(columns=existing_columns)\n",
    "    \n",
    "    if not final_df.empty:\n",
    "        output_path = os.path.join(partials_dir, f\"{symbol}.csv\")\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "\n",
    "async def main_fetcher_resumable():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"API_KEY not found.\")\n",
    "        return\n",
    "\n",
    "    partials_dir = '../output/Financial Statements/partials'\n",
    "    os.makedirs(partials_dir, exist_ok=True)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        print(\"--- Running pre-flight API check... ---\")\n",
    "        test_symbol = 'AAPL'\n",
    "        test_url = f\"https://financialmodelingprep.com/stable/income-statement\"\n",
    "        test_params = {\"symbol\": test_symbol, \"period\": \"quarter\", \"limit\": 1, \"apikey\": api_key}\n",
    "        test_data = await fetch_endpoint(session, test_url, test_params, test_symbol)\n",
    "        if not test_data:\n",
    "            print(f\"--- Pre-flight check FAILED. Could not fetch data for {test_symbol}. Please check your API key and network connection. ---\")\n",
    "            return\n",
    "        print(\"--- Pre-flight check PASSED. ---\")\n",
    "\n",
    "        symbols_df = pd.read_csv('../output/fetchable_symbols.csv')\n",
    "        all_symbols = symbols_df['symbol'].unique().tolist()\n",
    "        \n",
    "        completed_symbols = {f.split('.')[0] for f in os.listdir(partials_dir) if f.endswith('.csv')}\n",
    "        symbols_to_fetch = [s for s in all_symbols if s not in completed_symbols]\n",
    "        \n",
    "        print(f\"Found {len(all_symbols)} total symbols. {len(completed_symbols)} already completed.\")\n",
    "        print(f\"Fetching financial data for {len(symbols_to_fetch)} new symbols...\")\n",
    "\n",
    "        sem = asyncio.Semaphore(20)\n",
    "        async def fetch_with_semaphore(symbol):\n",
    "            async with sem:\n",
    "                await fetch_and_process_symbol(session, symbol, api_key, partials_dir)\n",
    "\n",
    "        tasks = [fetch_with_semaphore(symbol) for symbol in symbols_to_fetch]\n",
    "        \n",
    "        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Fetching Financials\"):\n",
    "            await f\n",
    "\n",
    "    print(\"--- Financial data fetching complete. ---\")\n",
    "\n",
    "# Run the main fetching process\n",
    "await main_fetcher_resumable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da978be7",
   "metadata": {},
   "source": [
    "## Assemble Final Financial Reports\n",
    "\n",
    "After the fetching process is complete, run this cell to combine all the individual symbol files from the `partials` directory into the four final alphabetized CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def assemble_final_reports():\n",
    "    \"\"\"\n",
    "    Assembles individual symbol CSVs from the 'partials' directory\n",
    "    into four final alphabetized batch files.\n",
    "    \"\"\"\n",
    "    partials_dir = '../output/Financial Statements/partials'\n",
    "    output_dir = '../output/Financial Statements'\n",
    "    \n",
    "    if not os.path.exists(partials_dir):\n",
    "        print(f\"Directory with partial files not found: {partials_dir}\")\n",
    "        return\n",
    "\n",
    "    all_files = [os.path.join(partials_dir, f) for f in os.listdir(partials_dir) if f.endswith('.csv')]\n",
    "    if not all_files:\n",
    "        print(\"No partial symbol files were found to assemble.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Assembling data from {len(all_files)} individual symbol files...\")\n",
    "    \n",
    "    # Use a generator to save memory\n",
    "    df_generator = (pd.read_csv(f) for f in all_files)\n",
    "    full_df = pd.concat(df_generator, ignore_index=True)\n",
    "    \n",
    "    if 'symbol' not in full_df.columns:\n",
    "        print(\"Error: 'symbol' column not found in the assembled data. Cannot create batches.\")\n",
    "        return\n",
    "\n",
    "    # Split into alphabetical chunks\n",
    "    symbol_chunks = {\n",
    "        \"A-F\": full_df[full_df['symbol'].str[0].str.upper() <= 'F'],\n",
    "        \"G-L\": full_df[(full_df['symbol'].str[0].str.upper() >= 'G') & (full_df['symbol'].str[0].str.upper() <= 'L')],\n",
    "        \"M-R\": full_df[(full_df['symbol'].str[0].str.upper() >= 'M') & (full_df['symbol'].str[0].str.upper() <= 'R')],\n",
    "        \"S-Z\": full_df[full_df['symbol'].str[0].str.upper() >= 'S']\n",
    "    }\n",
    "\n",
    "    for chunk_name, chunk_df in symbol_chunks.items():\n",
    "        if not chunk_df.empty:\n",
    "            output_path = os.path.join(output_dir, f\"financials_{chunk_name}.csv\")\n",
    "            chunk_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved chunk '{chunk_name}' with {len(chunk_df)} records to {output_path}\")\n",
    "        else:\n",
    "            print(f\"No data for chunk '{chunk_name}'.\")\n",
    "    \n",
    "    print(\"--- Assembly of final reports is complete. ---\")\n",
    "\n",
    "\n",
    "# Run the assembly function\n",
    "assemble_final_reports()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
