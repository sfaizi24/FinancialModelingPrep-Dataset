{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebab1c6",
   "metadata": {},
   "source": [
    "## Fetching Financial Statements (0-10 Years Ago)\n",
    "\n",
    "This next step will fetch 10 years of quarterly financial data for every symbol in our `fetchable_symbols.csv` list. This is a long-running process, but it has been redesigned to be **robust and resumable**.\n",
    "\n",
    "Hereâ€™s how it works:\n",
    "1.  **Pre-Flight Check:** It first runs a quick test on a single symbol to ensure the API is responding correctly.\n",
    "2.  **Individual Symbol Processing:** It fetches, processes, and saves the data for one symbol at a time to a temporary `partials` folder.\n",
    "3.  **Automatic Resuming:** If the script is stopped and restarted, it will automatically skip any symbols it has already completed, picking up where it left off.\n",
    "4.  **Clear Error Logging:** If a symbol fails repeatedly, it will now print a clear error message instead of failing silently.\n",
    "\n",
    "After this cell completes, a final script in the next cell will assemble these individual files into the four alphabetized batch files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_endpoint(session, url, params, symbol_for_logging):\n",
    "    \"\"\"Asynchronously fetches data from a single API endpoint with retries and logging.\"\"\"\n",
    "    retries = 3\n",
    "    last_error = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=30) as response:\n",
    "                if response.status == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", 5))\n",
    "                    await asyncio.sleep(retry_after)\n",
    "                    continue\n",
    "                response.raise_for_status()\n",
    "                return await response.json()\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            if i < retries - 1:\n",
    "                await asyncio.sleep((i + 1) * 2)\n",
    "            else:\n",
    "                endpoint_name = url.split('/')[-1]\n",
    "                tqdm.write(f\"ERROR for {symbol_for_logging}: Failed to fetch {endpoint_name} data after {retries} retries. Final error: {last_error}\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "async def fetch_and_process_symbol(session, symbol, api_key, partials_dir):\n",
    "    \"\"\"Fetches, merges, and saves data for a single symbol.\"\"\"\n",
    "    base_url = \"https://financialmodelingprep.com/stable\"\n",
    "    base_params = {\"period\": \"quarter\", \"limit\": 40, \"apikey\": api_key, \"symbol\": symbol}\n",
    "    \n",
    "    income_url = f\"{base_url}/income-statement\"\n",
    "    balance_url = f\"{base_url}/balance-sheet-statement\"\n",
    "    ratios_url = f\"{base_url}/ratios\"\n",
    "    \n",
    "    tasks = [\n",
    "        fetch_endpoint(session, income_url, base_params, symbol),\n",
    "        fetch_endpoint(session, balance_url, base_params, symbol),\n",
    "        fetch_endpoint(session, ratios_url, base_params, symbol)\n",
    "    ]\n",
    "    income_data, balance_data, ratios_data = await asyncio.gather(*tasks)\n",
    "\n",
    "    if not income_data and not balance_data and not ratios_data:\n",
    "        return\n",
    "\n",
    "    income_df = pd.DataFrame(income_data)\n",
    "    balance_df = pd.DataFrame(balance_data)\n",
    "    ratios_df = pd.DataFrame(ratios_data)\n",
    "    \n",
    "    all_dfs = []\n",
    "    if not income_df.empty: all_dfs.append(income_df.set_index(['date', 'symbol', 'period']))\n",
    "    if not balance_df.empty: all_dfs.append(balance_df.set_index(['date', 'symbol', 'period']))\n",
    "    if not ratios_df.empty: all_dfs.append(ratios_df.set_index(['date', 'symbol', 'period']))\n",
    "\n",
    "    if not all_dfs:\n",
    "        return\n",
    "\n",
    "    merged_df = pd.concat(all_dfs, axis=1).reset_index()\n",
    "    \n",
    "    columns_to_keep = {\n",
    "        'date': 'exactDate', 'symbol': 'symbol', 'revenue': 'revenue', 'netIncome': 'netIncome',\n",
    "        'ebitda': 'ebitda', 'eps': 'eps', 'priceToEarningsRatio': 'pToE', 'totalCurrentAssets': 'totalCurrentAssets',\n",
    "        'totalAssets': 'totalAssets', 'totalLiabilities': 'totalLiabilities', 'totalDebt': 'totalDebt',\n",
    "        'debtToAssetsRatio': 'debtToAssets', 'priceToBookRatio': 'pToB', 'debtToEquityRatio': 'debtToEquity',\n",
    "        'debtToCapitalRatio': 'debtToCapital', 'revenuePerShare': 'revenuePerShare'\n",
    "    }\n",
    "    \n",
    "    # Filter for columns that actually exist and drop duplicates from the join\n",
    "    merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "    existing_columns = {k: v for k, v in columns_to_keep.items() if k in merged_df.columns}\n",
    "    \n",
    "    if not existing_columns:\n",
    "        return\n",
    "\n",
    "    final_df = merged_df[list(existing_columns.keys())].rename(columns=existing_columns)\n",
    "    \n",
    "    if not final_df.empty:\n",
    "        output_path = os.path.join(partials_dir, f\"{symbol}.csv\")\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "\n",
    "async def main_fetcher_resumable():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"API_KEY not found.\")\n",
    "        return\n",
    "\n",
    "    partials_dir = '../output/Financial Statements/partials'\n",
    "    os.makedirs(partials_dir, exist_ok=True)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        print(\"--- Running pre-flight API check... ---\")\n",
    "        test_symbol = 'AAPL'\n",
    "        test_url = f\"https://financialmodelingprep.com/stable/income-statement\"\n",
    "        test_params = {\"symbol\": test_symbol, \"period\": \"quarter\", \"limit\": 1, \"apikey\": api_key}\n",
    "        test_data = await fetch_endpoint(session, test_url, test_params, test_symbol)\n",
    "        if not test_data:\n",
    "            print(f\"--- Pre-flight check FAILED. Could not fetch data for {test_symbol}. Please check your API key and network connection. ---\")\n",
    "            return\n",
    "        print(\"--- Pre-flight check PASSED. ---\")\n",
    "\n",
    "        symbols_df = pd.read_csv('../output/fetchable_symbols.csv')\n",
    "        all_symbols = symbols_df['symbol'].unique().tolist()\n",
    "        \n",
    "        completed_symbols = {f.split('.')[0] for f in os.listdir(partials_dir) if f.endswith('.csv')}\n",
    "        symbols_to_fetch = [s for s in all_symbols if s not in completed_symbols]\n",
    "        \n",
    "        print(f\"Found {len(all_symbols)} total symbols. {len(completed_symbols)} already completed.\")\n",
    "        print(f\"Fetching financial data for {len(symbols_to_fetch)} new symbols...\")\n",
    "\n",
    "        sem = asyncio.Semaphore(20)\n",
    "        async def fetch_with_semaphore(symbol):\n",
    "            async with sem:\n",
    "                await fetch_and_process_symbol(session, symbol, api_key, partials_dir)\n",
    "\n",
    "        tasks = [fetch_with_semaphore(symbol) for symbol in symbols_to_fetch]\n",
    "        \n",
    "        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Fetching Financials\"):\n",
    "            await f\n",
    "\n",
    "    print(\"--- Financial data fetching complete. ---\")\n",
    "\n",
    "# Run the main fetching process\n",
    "await main_fetcher_resumable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da978be7",
   "metadata": {},
   "source": [
    "## Assemble Final Financial Reports\n",
    "\n",
    "After the fetching process is complete, run this cell to combine all the individual symbol files from the `partials` directory into the four final alphabetized CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def assemble_final_reports():\n",
    "    \"\"\"\n",
    "    Assembles individual symbol CSVs from the 'partials' directory\n",
    "    into four final alphabetized batch files.\n",
    "    \"\"\"\n",
    "    partials_dir = '../output/Financial Statements/partials'\n",
    "    output_dir = '../output/Financial Statements'\n",
    "    \n",
    "    if not os.path.exists(partials_dir):\n",
    "        print(f\"Directory with partial files not found: {partials_dir}\")\n",
    "        return\n",
    "\n",
    "    all_files = [os.path.join(partials_dir, f) for f in os.listdir(partials_dir) if f.endswith('.csv')]\n",
    "    if not all_files:\n",
    "        print(\"No partial symbol files were found to assemble.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Assembling data from {len(all_files)} individual symbol files...\")\n",
    "    \n",
    "    # Use a generator to save memory\n",
    "    df_generator = (pd.read_csv(f) for f in all_files)\n",
    "    full_df = pd.concat(df_generator, ignore_index=True)\n",
    "    \n",
    "    if 'symbol' not in full_df.columns:\n",
    "        print(\"Error: 'symbol' column not found in the assembled data. Cannot create batches.\")\n",
    "        return\n",
    "\n",
    "    # Split into alphabetical chunks\n",
    "    symbol_chunks = {\n",
    "        \"A-F\": full_df[full_df['symbol'].str[0].str.upper() <= 'F'],\n",
    "        \"G-L\": full_df[(full_df['symbol'].str[0].str.upper() >= 'G') & (full_df['symbol'].str[0].str.upper() <= 'L')],\n",
    "        \"M-R\": full_df[(full_df['symbol'].str[0].str.upper() >= 'M') & (full_df['symbol'].str[0].str.upper() <= 'R')],\n",
    "        \"S-Z\": full_df[full_df['symbol'].str[0].str.upper() >= 'S']\n",
    "    }\n",
    "\n",
    "    for chunk_name, chunk_df in symbol_chunks.items():\n",
    "        if not chunk_df.empty:\n",
    "            output_path = os.path.join(output_dir, f\"financials_{chunk_name}.csv\")\n",
    "            chunk_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved chunk '{chunk_name}' with {len(chunk_df)} records to {output_path}\")\n",
    "        else:\n",
    "            print(f\"No data for chunk '{chunk_name}'.\")\n",
    "    \n",
    "    print(\"--- Assembly of final reports is complete. ---\")\n",
    "\n",
    "\n",
    "# Run the assembly function\n",
    "assemble_final_reports()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9565e6",
   "metadata": {},
   "source": [
    "## Fetching Financial Statements (Last 50 Years)\n",
    "\n",
    "This cell will fetch 50 years of quarterly financial data for every symbol in our `fetchable_symbols.csv` list. This is a long-running process that is robust and resumable. It will produce the final financial statement files with an A-D, E-J, K-P, Q-Z split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1102840a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running pre-flight API check for KO... ---\n",
      "--- Pre-flight check PASSED. Found 160 records for KO. ---\n",
      "Found 12800 total symbols. 11144 already completed for 50-year fetch.\n",
      "Fetching 50-year financial data for 1656 new symbols...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cc0c9ca4304d2eab7aa3bb450ddc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 50-Year Financials:   0%|          | 0/1656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 50-year financial data fetching complete. ---\n",
      "Assembling 50-year data from 11147 individual symbol files...\n",
      "Total records before deduplication: 603279\n",
      "Total records after deduplication: 603248\n",
      "Saved chunk 'A-D' with 174236 records to Statement Data\\financials_A-D.csv\n",
      "Saved chunk 'E-J' with 125034 records to Statement Data\\financials_E-J.csv\n",
      "Saved chunk 'K-P' with 144851 records to Statement Data\\financials_K-P.csv\n",
      "Saved chunk 'Q-Z' with 159075 records to Statement Data\\financials_Q-Z.csv\n",
      "--- Assembly of 50-year reports is complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_endpoint_50_years(session, url, params, symbol_for_logging):\n",
    "    \"\"\"Asynchronously fetches data from a single API endpoint with retries and logging.\"\"\"\n",
    "    retries = 3\n",
    "    last_error = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=30) as response:\n",
    "                if response.status == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", 5))\n",
    "                    await asyncio.sleep(retry_after)\n",
    "                    continue\n",
    "                response.raise_for_status()\n",
    "                return await response.json()\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            if i < retries - 1:\n",
    "                await asyncio.sleep((i + 1) * 2)\n",
    "            else:\n",
    "                endpoint_name = url.split('/')[-1]\n",
    "                tqdm.write(f\"ERROR for {symbol_for_logging}: Failed to fetch {endpoint_name} data after {retries} retries. Final error: {last_error}\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "async def fetch_and_process_symbol_50_years(session, symbol, api_key, partials_dir):\n",
    "    \"\"\"Fetches, merges, and saves 50 years of data for a single symbol.\"\"\"\n",
    "    base_url = \"https://financialmodelingprep.com/stable\"\n",
    "    \n",
    "    # Define date range for last 50 years\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    fifty_years_ago = (datetime.now() - timedelta(days=365*50)).strftime('%Y-%m-%d')\n",
    "\n",
    "    base_params = {\"period\": \"quarter\", \"from\": fifty_years_ago, \"to\": today, \"limit\": 200, \"apikey\": api_key, \"symbol\": symbol}\n",
    "    \n",
    "    income_url = f\"{base_url}/income-statement\"\n",
    "    balance_url = f\"{base_url}/balance-sheet-statement\"\n",
    "    ratios_url = f\"{base_url}/ratios\"\n",
    "    \n",
    "    tasks = [\n",
    "        fetch_endpoint_50_years(session, income_url, base_params, symbol),\n",
    "        fetch_endpoint_50_years(session, balance_url, base_params, symbol),\n",
    "        fetch_endpoint_50_years(session, ratios_url, base_params, symbol)\n",
    "    ]\n",
    "    income_data, balance_data, ratios_data = await asyncio.gather(*tasks)\n",
    "\n",
    "    if not income_data and not balance_data and not ratios_data:\n",
    "        return\n",
    "\n",
    "    income_df = pd.DataFrame(income_data)\n",
    "    balance_df = pd.DataFrame(balance_data)\n",
    "    ratios_df = pd.DataFrame(ratios_data)\n",
    "    \n",
    "    # Filter out any data with future dates\n",
    "    today_dt = pd.to_datetime(datetime.now().date())\n",
    "    if not income_df.empty:\n",
    "        income_df['date'] = pd.to_datetime(income_df['date'])\n",
    "        income_df = income_df[income_df['date'] <= today_dt]\n",
    "    if not balance_df.empty:\n",
    "        balance_df['date'] = pd.to_datetime(balance_df['date'])\n",
    "        balance_df = balance_df[balance_df['date'] <= today_dt]\n",
    "    if not ratios_df.empty:\n",
    "        ratios_df['date'] = pd.to_datetime(ratios_df['date'])\n",
    "        ratios_df = ratios_df[ratios_df['date'] <= today_dt]\n",
    "\n",
    "    all_dfs = []\n",
    "    if not income_df.empty: all_dfs.append(income_df.set_index(['date', 'symbol', 'period']))\n",
    "    if not balance_df.empty: all_dfs.append(balance_df.set_index(['date', 'symbol', 'period']))\n",
    "    if not ratios_df.empty: all_dfs.append(ratios_df.set_index(['date', 'symbol', 'period']))\n",
    "\n",
    "    if not all_dfs:\n",
    "        return\n",
    "\n",
    "    merged_df = pd.concat(all_dfs, axis=1).reset_index()\n",
    "    \n",
    "    columns_to_keep = {\n",
    "        'date': 'exactDate', 'symbol': 'symbol', 'revenue': 'revenue', 'netIncome': 'netIncome',\n",
    "        'ebitda': 'ebitda', 'eps': 'eps', 'priceToEarningsRatio': 'pToE', 'totalCurrentAssets': 'totalCurrentAssets',\n",
    "        'totalAssets': 'totalAssets', 'totalLiabilities': 'totalLiabilities', 'totalDebt': 'totalDebt',\n",
    "        'debtToAssetsRatio': 'debtToAssets', 'priceToBookRatio': 'pToB', 'debtToEquityRatio': 'debtToEquity',\n",
    "        'debtToCapitalRatio': 'debtToCapital', 'revenuePerShare': 'revenuePerShare',\n",
    "        'period': 'quarter', 'grossProfit': 'grossProfit', 'stockholdersEquity': 'stockholdersEquity'\n",
    "    }\n",
    "    \n",
    "    merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "    existing_columns = {k: v for k, v in columns_to_keep.items() if k in merged_df.columns}\n",
    "    \n",
    "    if not existing_columns:\n",
    "        return\n",
    "\n",
    "    final_df = merged_df[list(existing_columns.keys())].rename(columns=existing_columns)\n",
    "    \n",
    "    if not final_df.empty:\n",
    "        output_path = os.path.join(partials_dir, f\"{symbol}.csv\")\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "\n",
    "async def main_fetcher_50_years_resumable():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"API_KEY not found.\")\n",
    "        return\n",
    "\n",
    "    partials_dir = 'Statement Data/partials_50_years'\n",
    "    os.makedirs(partials_dir, exist_ok=True)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Pre-flight check for KO to ensure the 'limit' parameter is working\n",
    "        print(\"--- Running pre-flight API check for KO... ---\")\n",
    "        test_symbol = 'KO'\n",
    "        test_url = \"https://financialmodelingprep.com/stable/income-statement\"\n",
    "        test_params = {\"period\": \"quarter\", \"limit\": 200, \"apikey\": api_key, \"symbol\": test_symbol}\n",
    "        test_data = await fetch_endpoint_50_years(session, test_url, test_params, test_symbol)\n",
    "        \n",
    "        if not test_data or len(test_data) <= 40:\n",
    "            print(f\"--- Pre-flight check FAILED. Expected >40 records for {test_symbol} but got {len(test_data)}. Check API 'limit' parameter. ---\")\n",
    "            return\n",
    "        print(f\"--- Pre-flight check PASSED. Found {len(test_data)} records for KO. ---\")\n",
    "\n",
    "        symbols_df = pd.read_csv('../Ticker Symbols/Symbol Lists/fetchable_symbols.csv')\n",
    "        all_symbols = symbols_df['symbol'].unique().tolist()\n",
    "        \n",
    "        completed_symbols = {f.split('.')[0] for f in os.listdir(partials_dir) if f.endswith('.csv')}\n",
    "        symbols_to_fetch = [s for s in all_symbols if s not in completed_symbols]\n",
    "        \n",
    "        if not symbols_to_fetch:\n",
    "            print(\"All required symbols have already been processed.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(all_symbols)} total symbols. {len(completed_symbols)} already completed for 50-year fetch.\")\n",
    "        print(f\"Fetching 50-year financial data for {len(symbols_to_fetch)} new symbols...\")\n",
    "\n",
    "        # --- Rate Limiting and Concurrency Control ---\n",
    "        API_CALL_INTERVAL = 0.077\n",
    "        CONCURRENCY_LIMIT = 20\n",
    "        sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "        \n",
    "        with tqdm(total=len(symbols_to_fetch), desc=\"Fetching 50-Year Financials\") as pbar:\n",
    "            async def fetch_with_sem(symbol):\n",
    "                async with sem:\n",
    "                    await fetch_and_process_symbol_50_years(session, symbol, api_key, partials_dir)\n",
    "                pbar.update(1)\n",
    "\n",
    "            tasks = []\n",
    "            for symbol in symbols_to_fetch:\n",
    "                tasks.append(asyncio.create_task(fetch_with_sem(symbol)))\n",
    "                await asyncio.sleep(API_CALL_INTERVAL)\n",
    "            \n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"--- 50-year financial data fetching complete. ---\")\n",
    "\n",
    "def assemble_50_years_reports():\n",
    "    partials_dir = 'Statement Data/partials_50_years'\n",
    "    output_dir = 'Statement Data'\n",
    "    \n",
    "    if not os.path.exists(partials_dir):\n",
    "        print(f\"Directory with partial files not found: {partials_dir}\")\n",
    "        return\n",
    "\n",
    "    all_files = [os.path.join(partials_dir, f) for f in os.listdir(partials_dir) if f.endswith('.csv')]\n",
    "    if not all_files:\n",
    "        print(\"No partial 50-year symbol files were found to assemble.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Assembling 50-year data from {len(all_files)} individual symbol files...\")\n",
    "    \n",
    "    df_generator = (pd.read_csv(f) for f in all_files)\n",
    "    full_df = pd.concat(df_generator, ignore_index=True)\n",
    "    \n",
    "    if 'symbol' not in full_df.columns:\n",
    "        print(\"Error: 'symbol' column not found in the assembled data. Cannot create batches.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Total records before deduplication: {len(full_df)}\")\n",
    "\n",
    "    # Deduplicate based on symbol and date\n",
    "    full_df.drop_duplicates(subset=['symbol', 'exactDate'], inplace=True)\n",
    "    \n",
    "    print(f\"Total records after deduplication: {len(full_df)}\")\n",
    "\n",
    "    new_symbol_chunks = {\n",
    "        \"A-D\": full_df[full_df['symbol'].str[0].str.upper() <= 'D'],\n",
    "        \"E-J\": full_df[(full_df['symbol'].str[0].str.upper() >= 'E') & (full_df['symbol'].str[0].str.upper() <= 'J')],\n",
    "        \"K-P\": full_df[(full_df['symbol'].str[0].str.upper() >= 'K') & (full_df['symbol'].str[0].str.upper() <= 'P')],\n",
    "        \"Q-Z\": full_df[full_df['symbol'].str[0].str.upper() >= 'Q']\n",
    "    }\n",
    "\n",
    "    for chunk_name, chunk_df in new_symbol_chunks.items():\n",
    "        if not chunk_df.empty:\n",
    "            output_path = os.path.join(output_dir, f\"financials_{chunk_name}.csv\")\n",
    "            chunk_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved chunk '{chunk_name}' with {len(chunk_df)} records to {output_path}\")\n",
    "        else:\n",
    "            print(f\"No data for chunk '{chunk_name}'.\")\n",
    "    \n",
    "    print(\"--- Assembly of 50-year reports is complete. ---\")\n",
    "\n",
    "# Run the main fetching process and then assemble\n",
    "await main_fetcher_50_years_resumable()\n",
    "assemble_50_years_reports()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a8263",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
